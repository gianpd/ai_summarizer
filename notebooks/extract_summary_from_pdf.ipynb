{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.edmundson import EdmundsonSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizerFast\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_summary = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizerFast.from_pretrained(checkpoint_summary)\n",
    "model = BartForConditionalGeneration.from_pretrained(checkpoint_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "  'model': model, \n",
    "  'max_length': 512,\n",
    "  'min_length': 120\n",
    "}\n",
    "\n",
    "tokenizer_dict = {\n",
    "  'tokenizer': tokenizer, \n",
    "  'max_length': 1024\n",
    "}\n",
    "\n",
    "\n",
    "def get_extractive_summary_from_text(text: str, language: str = 'ita', sentence_count: int = 15) -> str:\n",
    "  parser = PlaintextParser.from_string(text, Tokenizer(language))\n",
    "  stemmer = Stemmer(language)\n",
    "  # summarizer = LsaSummarizer(stemmer)\n",
    "  # summarizer.stop_words = get_stop_words(language)\n",
    "  summarizer = EdmundsonSummarizer(stemmer)\n",
    "  summarizer.null_words = get_stop_words(language)\n",
    "  summarizer.bonus_words = ['importante', 'incredibile']\n",
    "  summarizer.stigma_words = ['impossibile', 'difficile', 'complicato']\n",
    "  extractive_summary_from_url = ''.join([sent._text for sent in summarizer(parser.document, sentence_count)])\n",
    "  return extractive_summary_from_url\n",
    "\n",
    "def get_summary(text_content):\n",
    "  tokenizer = tokenizer_dict['tokenizer']\n",
    "  model = model_dict['model']\n",
    "\n",
    "  inputs = tokenizer(text_content, max_length=tokenizer_dict['max_length'], truncation=True, return_tensors=\"pt\")\n",
    "  outputs = model.generate(\n",
    "      inputs[\"input_ids\"], max_length=model_dict['max_length'], min_length=model_dict['min_length'], \n",
    "  )\n",
    "\n",
    "  summarized_text = tokenizer.decode(outputs[0])\n",
    "  match = re.search(r\"<s>(.*)</s>\", summarized_text)\n",
    "  if match is not None: summarized_text = match.group(1)\n",
    "\n",
    "  return summarized_text.replace('<s>', '').replace('</s>', '') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 = reader.pages[0].extract_text()\n",
    "# p1 = p1.split('Abstract')[1].split('1 Introduction')\n",
    "# abstract = p1[0]\n",
    "# intro = p1[1].split('2 Model')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"prova.pdf\"\n",
    "reader = PdfReader(fname)\n",
    "len(reader.pages)\n",
    "\n",
    "text = ' '.join([p.extract_text() for p in reader.pages])\n",
    "\n",
    "# LANG = 'ita'\n",
    "# parser = PlaintextParser.from_string(text, Tokenizer(LANG))\n",
    "# stemmer = Stemmer(LANG)\n",
    "# summarizer = Summarizer(stemmer)\n",
    "# summarizer.stop_words = get_stop_words(LANG)\n",
    "# s = [s._text for s in summarizer(parser.document, 20)]\n",
    "# len(s)\n",
    "# ss = ''.join(s)\n",
    "# ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"prova.pdf\"\n",
    "reader = PdfReader(fname)\n",
    "len(reader.pages)\n",
    "\n",
    "idxs = [\n",
    "    ('1 intro', 5, 17, '2 Sistema Traffico AS-IS '), \n",
    "    ('2 sistema traffico as-is', 17, 28, '3 Sistema Traffico TO-BE'), \n",
    "    ('3 sistema traffico to-be', 28, 46, '4 Oggetto della fornitura'),\n",
    "    ('4 oggetto della fornitura', 46, 51, '5 Figure e competenze richieste'),\n",
    "    ('5 figure e competenze richieste', 51, 57, '6 Condizioni generali del contratto')]\n",
    "\n",
    "total = {}\n",
    "for t in idxs:\n",
    "    total[t[0]] = {'original': '', 'extractive_summary': '', 'abstractive_summary': ''}\n",
    "    for i in range(t[1], t[2]):\n",
    "        page = reader.pages[i].extract_text()\n",
    "        page = page.replace(\"è\", \"e'\").replace(\"à\", \"a'\")\n",
    "        total[t[0]]['original'] += page\n",
    "        # total[t[0]]['original'] = total[t[0]]['original'].\\\n",
    "        #     replace('\\n\\n\\n\\n', '\\n').\\\n",
    "        #     replace('\\n\\n', '\\n').\\\n",
    "        #     replace('\\n', '').\\\n",
    "        total[t[0]]['original'] = total[t[0]]['original'].replace('ASPI Programma di ammodernamento  Area Traffico  Capitolato tecnico', '')\n",
    "        total[t[0]]['original'] = total[t[0]]['original'].split(t[-1])[0]\n",
    "        total[t[0]]['extractive_summary'] = get_extractive_summary_from_text(total[t[0]]['original'])\n",
    "        # total[t[0]]['abstractive_summary'] =  get_summary(total[t[0]]['extractive_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"exctractive_summary.txt\", '+w')\n",
    "for key in total.keys():\n",
    "    s = ' ----- ' + key.upper() + ' ------ ' + total[key]['extractive_summary']\n",
    "    f.write(s)\n",
    "f.close()\n",
    "\n",
    "# f = open(\"abstractive_summary.txt\", '+w')\n",
    "# for key in total.keys():\n",
    "#     s = ' ----- ' + key.upper() + ' ------ ' + total[key]['abstractive_summary']\n",
    "#     f.write(s)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b93850bb842f8947f316d4abbea9a1494c38451463ad7eee7d18c008579f1a94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
