{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/gianpd/develop/sumy/')\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.edmundson import EdmundsonSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizerFast\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_summary = \"facebook/bart-large-cnn\"\n",
    "# tokenizer = BartTokenizerFast.from_pretrained(checkpoint_summary)\n",
    "# model = BartForConditionalGeneration.from_pretrained(checkpoint_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dict = {\n",
    "#   'model': model, \n",
    "#   'max_length': 512,\n",
    "#   'min_length': 120\n",
    "# }\n",
    "\n",
    "# tokenizer_dict = {\n",
    "#   'tokenizer': tokenizer, \n",
    "#   'max_length': 1024\n",
    "# }\n",
    "\n",
    "\n",
    "def get_extractive_summary_from_text(text: str, language: str = 'italian', sentence_count: int = 15) -> str:\n",
    "  parser = PlaintextParser.from_string(text, Tokenizer(language))\n",
    "  stemmer = Stemmer(language)\n",
    "  summarizer = LsaSummarizer(stemmer)\n",
    "  summarizer.stop_words = get_stop_words(language)\n",
    "  extractive_summary_from_url = ''.join([sent._text for sent in summarizer(parser.document, sentence_count)])\n",
    "  return extractive_summary_from_url\n",
    "\n",
    "# def get_summary(text_content):\n",
    "#   tokenizer = tokenizer_dict['tokenizer']\n",
    "#   model = model_dict['model']\n",
    "\n",
    "#   inputs = tokenizer(text_content, max_length=tokenizer_dict['max_length'], truncation=True, return_tensors=\"pt\")\n",
    "#   outputs = model.generate(\n",
    "#       inputs[\"input_ids\"], max_length=model_dict['max_length'], min_length=model_dict['min_length'], \n",
    "#   )\n",
    "\n",
    "#   summarized_text = tokenizer.decode(outputs[0])\n",
    "#   match = re.search(r\"<s>(.*)</s>\", summarized_text)\n",
    "#   if match is not None: summarized_text = match.group(1)\n",
    "\n",
    "#   return summarized_text.replace('<s>', '').replace('</s>', '') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dei paragrafi\n",
    "1. avendo come inputs il nome del paragrafo di interesse e quello subito successivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"edmonson69.pdf\"\n",
    "reader = PdfReader(fname)\n",
    "len(reader.pages)\n",
    "\n",
    "page1 = reader.pages[0].extract_text()\n",
    "full_text = ' '.join([p.extract_text() for p in reader.pages])\n",
    "abstract = full_text.split('Introduction')[0].split('ABSTRACT')[-1]\n",
    "introduction = full_text.split('Research Methodology')[0].split('Introduction')[-1]\n",
    "abstract\n",
    "introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search(r\"([\\w.\\s])\", full_text)\n",
    "if match:\n",
    "    result = match.group()\n",
    "    print(result)\n",
    "else:\n",
    "    print('did not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs(full_text):\n",
    "    lines = full_text.splitlines(True)\n",
    "    paragraphs = []\n",
    "    for line in lines:\n",
    "        if line.isspace():\n",
    "            if paragraphs:\n",
    "                yield ''.join(paragraphs)\n",
    "                paragraphs = []\n",
    "        else:\n",
    "            paragraphs.append(line)\n",
    "    if paragraphs:\n",
    "        yield ''.join(paragraphs)\n",
    "\n",
    "\n",
    "paragraphs = get_paragraphs(full_text=full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = \"prova.pdf\"\n",
    "# reader = PdfReader(fname)\n",
    "# len(reader.pages)\n",
    "\n",
    "# idxs = [\n",
    "#     ('1 intro', 5, 17, '2 Sistema Traffico AS-IS '), \n",
    "#     ('2 sistema traffico as-is', 17, 28, '3 Sistema Traffico TO-BE'), \n",
    "#     ('3 sistema traffico to-be', 28, 46, '4 Oggetto della fornitura'),\n",
    "#     ('4 oggetto della fornitura', 46, 51, '5 Figure e competenze richieste'),\n",
    "#     ('5 figure e competenze richieste', 51, 57, '6 Condizioni generali del contratto')]\n",
    "\n",
    "# total = {}\n",
    "# for t in idxs:\n",
    "#     total[t[0]] = {'original': '', 'extractive_summary': '', 'abstractive_summary': ''}\n",
    "#     for i in range(t[1], t[2]):\n",
    "#         page = reader.pages[i].extract_text()\n",
    "#         page = page.replace(\"è\", \"e'\").replace(\"à\", \"a'\")\n",
    "#         total[t[0]]['original'] += page\n",
    "#         # total[t[0]]['original'] = total[t[0]]['original'].\\\n",
    "#         #     replace('\\n\\n\\n\\n', '\\n').\\\n",
    "#         #     replace('\\n\\n', '\\n').\\\n",
    "#         #     replace('\\n', '').\\\n",
    "#         total[t[0]]['original'] = total[t[0]]['original'].replace('ASPI Programma di ammodernamento  Area Traffico  Capitolato tecnico', '')\n",
    "#         total[t[0]]['original'] = total[t[0]]['original'].split(t[-1])[0]\n",
    "#         total[t[0]]['extractive_summary'] = get_extractive_summary_from_text(total[t[0]]['original'])\n",
    "#         # total[t[0]]['abstractive_summary'] =  get_summary(total[t[0]]['extractive_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"exctractive_summary.txt\", '+w')\n",
    "# for key in total.keys():\n",
    "#     s = ' ----- ' + key.upper() + ' ------ ' + total[key]['extractive_summary']\n",
    "#     f.write(s)\n",
    "# f.close()\n",
    "\n",
    "# # f = open(\"abstractive_summary.txt\", '+w')\n",
    "# # for key in total.keys():\n",
    "# #     s = ' ----- ' + key.upper() + ' ------ ' + total[key]['abstractive_summary']\n",
    "# #     f.write(s)\n",
    "# # f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b93850bb842f8947f316d4abbea9a1494c38451463ad7eee7d18c008579f1a94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
