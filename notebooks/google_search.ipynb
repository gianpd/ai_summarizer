{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizerFast\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_summary = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizerFast.from_pretrained(checkpoint_summary)\n",
    "model = BartForConditionalGeneration.from_pretrained(checkpoint_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lsa_extractive_summary(input_str: str, url: bool = True, sentence_count: Optional[int] = 15, language: Optional[str] = \"english\") -> str:\n",
    "    \"\"\"\"Get an exctractive summary using the LSA (Latent Semantic Analysys) algorithm from an URL or from a Text.\n",
    "    \n",
    "    --Parameters\n",
    "     - input_Str (str): the http url of the article to parse or a text.\n",
    "     - url (bool): if the input_str is an url or not.\n",
    "     - sentence_count (int): the number of sentences to extract.\n",
    "     - language (str): the used language for setting the stemmer and getting the stop words\n",
    "\n",
    "     return (str) the extractive summary as a string\n",
    "    \"\"\"\n",
    "    parser = HtmlParser.from_url(input_str, Tokenizer(language)) if url else PlaintextParser.from_string(input_str, Tokenizer(language))\n",
    "    stemmer = Stemmer(language)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(language)\n",
    "    extractive_summary = ' '.join([sent._text for sent in summarizer(parser.document, sentence_count)])\n",
    "    return extractive_summary\n",
    "\n",
    "def get_summary(dict_summarizer_model, dict_tokenizer, text_content):\n",
    "  # text_content = get_extractive_summary(text_content, EXTRACTED_ARTICLE_SENTENCES_LEN)\n",
    "  tokenizer = dict_tokenizer['tokenizer']\n",
    "  model = dict_summarizer_model['model']\n",
    "\n",
    "  inputs = tokenizer(text_content, max_length=dict_tokenizer['max_length'], truncation=True, return_tensors=\"pt\")\n",
    "  outputs = model.generate(\n",
    "      inputs[\"input_ids\"], max_length=dict_summarizer_model['max_length'], min_length=dict_summarizer_model['min_length'], \n",
    "  )\n",
    "\n",
    "  summarized_text = tokenizer.decode(outputs[0])\n",
    "  match = re.search(r\"<s>(.*)</s>\", summarized_text)\n",
    "  if match is not None: summarized_text = match.group(1)\n",
    "\n",
    "  return summarized_text.replace('<s>', '').replace('</s>', '') \n",
    "  \n",
    "\n",
    "model_dict = {\n",
    "  'model': model, \n",
    "  'max_length': 512,\n",
    "  'min_length': 120\n",
    "}\n",
    "\n",
    "tokenizer_dict = {\n",
    "  'tokenizer': tokenizer, \n",
    "  'max_length': 1024\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'bandi sotto i 200 mila euro start-up tecnologiche'\n",
    "results = [x for x in search(query=text, num=5, stop=5, pause=2, tbs=\"qdr:d\")]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = [(x, get_lsa_extractive_summary(input_str=x)) for x in results]\n",
    "summaries = list(filter(lambda x: len(x[1]) >= 700, summaries))\n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_summaries = [(x[0], get_summary(model_dict, tokenizer_dict, text_content=x[1])) if len(x[1]) > 800 else x for x in summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b93850bb842f8947f316d4abbea9a1494c38451463ad7eee7d18c008579f1a94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
